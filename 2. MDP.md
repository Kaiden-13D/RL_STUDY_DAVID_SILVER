# 🎓 [Study Note] David Silver 강화학습 제2강: MDP 정리

## 📑 목차
1. [Introduction](#0-introduction)
2. [Step 1. Markov Process (MP)](#1-step-1-markov-process-mp)
3. [Step 2. Markov Reward Process (MRP)](#2-step-2-markov-reward-process-mrp)
    * 2.1 Return ($G_t$)과 할인율 ($\gamma$)
    * 2.2 가치 함수 ($v(s)$)의 정의
    * 2.3 벨만 기대 방정식 (Bellman Expectation Equation)
4. [Step 3. Markov Decision Process (MDP)](#3-step-3-markov-decision-process-mdp)
    * 3.1 정책 (Policy, $\pi$)의 정의
    * 3.2 상태 가치($v_\pi$)와 행동 가치($q_\pi$)
    * 3.3 MDP에서의 벨만 기대 방정식
5. [Step 4. Optimal Value & Policy](#4-step-4-optimal-value--policy)
    * 4.1 최적 가치 함수 ($v_*, q_*$)
    * 4.2 최적 정책 ($\pi_*$)과 벨만 최적 방정식

---

<a id="0-introduction"></a>
## 0. Introduction
* MDP는 강화학습 환경을 수학적으로 공식화하는 도구임.
* 환경이 **Fully Observable**하여 현재 상태가 미래를 결정하는 데 필요한 모든 정보를 담고 있다고 가정함.
* 거의 모든 RL 문제는 MDP로 공식화 가능함.

---

<a id="1-step-1-markov-process-mp"></a>
## 1. Step 1: Markov Process (MP) - "상태 전이의 기초"
* 가장 기본 단계로, 행동(Action)과 보상(Reward)이 없는 단순 상태의 나열임.
* **마르코프 성질 (Markov Property)**: "미래는 과거와 독립적이며, 오직 현재에 의해서만 결정됨."
    $$P[S_{t+1} | S_t] = P[S_{t+1} | S_1, \dots, S_t]$$
* **상태 전이 행렬 (State Transition Matrix, $P$)**: 모든 상태 $s$에서 다음 상태 $s'$로 갈 확률을 정리한 $n \times n$ 행렬임. 각 행의 합은 항상 1임.
    $$P_{ss'} = P[S_{t+1} = s' | S_t = s]$$

---

<a id="2-step-2-markov-reward-process-mrp"></a>
## 2. Step 2: Markov Reward Process (MRP) - "가치 평가의 시작"
* MP에 보상($R$)과 할인율($\gamma$)이 추가된 구조임.
* 에이전트는 선택권이 없으며, 환경이 이끄는 대로 가며 보상만 받음.

### 2.1 Return ($G_t$)과 할인율 ($\gamma$)
* **Return ($G_t$)**: 시점 $t$부터 게임 종료까지 받을 누적 보상의 합임.
    $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
* **할인율 ($\gamma \in [0, 1]$)**: 미래 보상을 현재 가치로 환산하는 값임.
    * $\gamma=0$: 매우 근시안적(Myopic). 당장 눈앞의 보상만 중시함.
    * $\gamma=1$: 매우 원시안적(Far-sighted). 미래 보상을 현재와 똑같이 중시함.
* **사용 이유**: 수학적 수렴성 보장, 미래의 불확실성 반영 등.

### 2.2 가치 함수 (Value Function, $v(s)$)
* 상태 $s$에 서 있을 때, 앞으로 받을 Return의 기대값임.
    $$v(s) = E[G_t | S_t = s]$$

### 2.3 벨만 기대 방정식 (Bellman Expectation Equation)
* 현재 상태 가치를 즉각 보상과 다음 상태 가치의 합으로 쪼개는 재귀적 수식임.
    $$v(s) = E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]$$
* 이를 확률을 사용하여 전개하면 다음과 같음:
    $$v(s) = R_s + \gamma \sum_{s' \in S} P_{ss'} v(s')$$
* **시스템적 해법 (Matrix Form)**: 
    $$v = R + \gamma P v \implies v = (I - \gamma P)^{-1} R$$
    * 역행렬 계산 복잡도는 $O(n^3)$으로 상태가 적은 경우에만 사용함.

---

<a id="3-step-3-markov-decision-process-mdp"></a>
## 3. Step 3: Markov Decision Process (MDP) - "의사결정의 도입"
* 에이전트가 행동(Action)을 선택할 수 있게 됨.

### 3.1 정책 (Policy, $\pi$)
* 상태 $s$에서 행동 $a$를 취할 확률 분포임.
    $$\pi(a|s) = P[A_t = a | S_t = s]$$
* 정책은 현재 상태에만 의존함 (**Stationary**). 정책이 결정되면 MDP는 MRP로 변환됨.

### 3.2 두 가지 가치 함수
* **상태 가치 함수 ($v_\pi(s)$)**: 정책 $\pi$를 따를 때, 상태 $s$의 기대 수익임.
    $$v_\pi(s) = E_\pi [G_t | S_t = s]$$
* **행동 가치 함수 ($q_\pi(s, a)$)**: 상태 $s$에서 행동 $a$를 취하고, 그 이후 정책 $\pi$를 따를 때의 기대 수익임.
    $$q_\pi(s, a) = E_\pi [G_t | S_t = s, A_t = a]$$

### 3.3 MDP에서의 벨만 기대 방정식
* **$v_\pi$와 $q_\pi$의 관계**:
    $$v_\pi(s) = \sum_{a \in A} \pi(a|s) q_\pi(s, a)$$
    $$q_\pi(s, a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_\pi(s')$$
* **통합 재귀식**:
    $$v_\pi(s) = \sum_{a \in A} \pi(a|s) (R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_\pi(s'))$$

---

<a id="4-step-4-optimal-value--policy"></a>
## 4. Step 4: Optimal Value & Policy - "최적의 길 찾기"
* 목표는 모든 정책 중 누적 보상을 최대화하는 **최적 정책($\pi_*$)**을 찾는 것임.

### 4.1 최적 가치 함수 ($v_*, q_*$)
* $v_*(s) = \max_\pi v_\pi(s)$: 모든 정책 중 상태 $s$에서 얻을 수 있는 최대 가치임.
* $q_*(s, a) = \max_\pi q_\pi(s, a)$: 모든 정책 중 $(s, a)$에서 얻을 수 있는 최대 가치임.

### 4.2 최적 정책 ($\pi_*$)과 벨만 최적 방정식
* **Finding $\pi_*$**: $q_*(s, a)$를 안다면 가장 높은 $q_*$ 값을 주는 행동을 100% 선택함.
    $$\pi_*(a|s) = 1 \text{ if } a = \text{argmax}_{a \in A} q_*(s, a), \text{ else } 0$$
* **벨만 최적 방정식 ($v_*$)**: 평균 대신 **최대값(max)**을 취함.
    $$v_*(s) = \max_a q_*(s, a) = \max_a (R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_*(s'))$$
* **벨만 최적 방정식 ($q_*$)**:
    $$q_*(s, a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a \max_{a'} q_*(s', a')$$

> **[주의]**
> 벨만 최적 방정식은 **max** 연산자 때문에 **비선형(Non-linear)**임. 역행렬로 한 번에 풀 수 없으며, Value Iteration 등 반복 알고리즘으로 풀어야 함.