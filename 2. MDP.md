# 🎓 [Study Note] David Silver 강화학습 제2강: MDP 정리

## 📑 목차
1. **[Introduction](#0-introduction)**: MDP의 정의와 목적
2. **[Step 1. Markov Process (MP)](#1-step-1-markov-process-mp---상태-전이의-기초)**: state transition의 기초
3. **[Step 2. Markov Reward Process (MRP)](#2-step-2-markov-reward-process-mrp---가치-평가의-시작)**: 가치 평가의 시작
    * [2.1 Return ($G_t$)과 할인율 ($\gamma$)](#21-return)
    * [2.2 가치 함수 ($v(s)$)의 정의](#22-value-function)
    * [2.3 벨만 기대 방정식 (Bellman Expectation Equation)](#23-bellman-expectation)
4. **[Step 3. Markov Decision Process (MDP)](#3-step-3-markov-decision-process-mdp---의사결정의-도입)**: 의사결정의 도입
    * [3.1 정책 (Policy, $\pi$)의 정의](#31-policy)
    * [3.2 상태 가치($v_\pi$)와 행동 가치($q_\pi$)](#32-two-value-functions)
    * [3.3 MDP에서의 벨만 기대 방정식](#33-mdp-bellman)
5. **[Step 4. Optimal Value & Policy](#4-step-4-optimal-value--policy---최적의-길-찾기)**: 최적의 길 찾기
    * [4.1 최적 가치 함수 ($v_*, q_*$)](#41-optimal-value)
    * [4.2 최적 정책 ($\pi_*$)과 벨만 최적 방정식](#42-optimal-policy)

---

## 0. Introduction
* MDP는 강화학습 환경을 수학적으로 공식화하는 도구임.
* 환경이 **Fully Observable**하여 현재 상태가 미래를 결정하는 데 필요한 모든 정보를 담고 있다고 가정함.
* 거의 모든 강화학습 문제는 MDP로 공식화 가능함.

---

## 1. Step 1: Markov Process (MP) - "상태 전이의 기초"
* 가장 기본 단계로, 행동(Action)과 보상(Reward)이 없는 단순 상태의 나열임.
* **마르코프 성질 (Markov Property)**: "미래는 과거와 독립적이며, 오직 현재에 의해서만 결정됨."
    $$\mathbb{P}[S_{t+1} | S_t] = \mathbb{P}[S_{t+1} | S_1, \dots, S_t]$$
* **상태 전이 행렬 (State Transition Matrix, $\mathcal{P}$)**: 모든 상태 $s$에서 다음 상태 $s'$로 갈 확률을 정리한 $n \times n$ 행렬임. 각 행의 합은 항상 1임.
    $$\mathcal{P}_{ss'} = \mathbb{P}[S_{t+1} = s' | S_t = s]$$

---

## 2. Step 2: Markov Reward Process (MRP) - "가치 평가의 시작"
* MP에 보상($R$)과 할인율($\gamma$)이 추가된 구조임.
* 에이전트는 선택권이 없으며, 환경이 이끄는 대로 가며 보상만 받음.

<a id="21-return"></a>
### 2.1 Return ($G_t$)과 할인율 ($\gamma$)
* **Return ($G_t$)**: 시점 $t$부터 게임 종료까지 받을 누적 보상의 합임.
    $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
* **할인율 ($\gamma \in [0, 1]$)**: 미래 보상을 현재 가치로 환산하는 값임.
    * $\gamma=0$: 매우 근시안적(Myopic). 당장 눈앞의 보상만 중시함.
    * $\gamma=1$: 매우 원시안적(Far-sighted). 미래 보상을 현재와 똑같이 중시함.
* **사용 이유**: 수학적 수렴성 보장, 미래의 불확실성 반영, 금융적 이자 개념 등.

<a id="22-value-function"></a>
### 2.2 가치 함수 (Value Function, $v(s)$)
* 상태 $s$에 서 있을 때, 앞으로 받을 Return의 기대값임.
    $$v(s) = \mathbb{E}[G_t | S_t = s]$$

<a id="23-bellman-expectation"></a>
### 2.3 벨만 기대 방정식 (Bellman Expectation Equation)
* 현재 상태 가치를 즉각 보상과 다음 상태 가치의 합으로 쪼개는 재귀적 수식임.
    $$v(s) = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]$$
* 이를 확률을 사용하여 전개하면 다음과 같음:
    $$v(s) = \mathcal{R}_s + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'} v(s')$$
    * $\mathcal{R}_s$: 상태 $s$에서의 즉각 보상.
    * $\gamma \sum \mathcal{P} v$: 다음 상태 $s'$들의 가치를 확률적으로 가중 평균 낸 값.

> **[시스템적 해법] 행렬식 표현 및 역행렬 계산**
> $$v = \mathcal{R} + \gamma \mathcal{P} v \implies v = (I - \gamma \mathcal{P})^{-1} \mathcal{R}$$
> * $O(n^3)$의 복잡도를 가지므로 상태가 적은 경우에만 직접 계산(Direct solution) 가능함.

---

## 3. Step 3: Markov Decision Process (MDP) - "의사결정의 도입"
* 에이전트가 행동(Action)을 선택할 수 있게 됨.

<a id="31-policy"></a>
### 3.1 정책 (Policy, $\pi$)
* 상태 $s$에서 행동 $a$를 취할 확률로, 에이전트의 전략 그 자체임.
    $$\pi(a|s) = \mathbb{P}[A_t = a | S_t = s]$$
* 정책은 시간에 의존하지 않고 현재 상태에만 의존함 (**Stationary**).
* 정책이 결정되면 MDP는 다시 MRP($\langle \mathcal{S}, \mathcal{P}^\pi, \mathcal{R}^\pi, \gamma \rangle$)로 변환되어 분석 가능함.

<a id="32-two-value-functions"></a>
### 3.2 두 가지 가치 함수
* **상태 가치 함수 ($v_\pi(s)$)**: 정책 $\pi$를 따를 때, 상태 $s$의 기대 수익임.
    $$v_\pi(s) = \mathbb{E}_\pi [G_t | S_t = s]$$
* **행동 가치 함수 ($q_\pi(s, a)$)**: 상태 $s$에서 행동 $a$를 취하고, 그 이후 정책 $\pi$를 따를 때의 기대 수익(성적표의 세부 항목)임.
    $$q_\pi(s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a]$$

<a id="33-mdp-bellman"></a>
### 3.3 MDP에서의 벨만 기대 방정식
* **$v_\pi(s)$를 $q_\pi$로 표현**: 선택 가능한 행동들의 가치를 정책 확률로 가중 평균함.
    $$v_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) q_\pi(s, a)$$
* **$q_\pi(s, a)$를 $v_\pi$로 표현**: 행동 취득 즉각 보상과 다음 칸($s'$)들의 가치를 평균 냄.
    $$q_\pi(s, a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_\pi(s')$$
* **최종 재귀식**: 위 두 식을 합쳐 $v_\pi(s)$를 위한 재귀식 완성함.
    $$v_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \left( \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_\pi(s') \right)$$

---

## 4. Step 4: Optimal Value & Policy - "최적의 길 찾기"
* 최종 목표는 모든 정책 중 가장 보상을 많이 받는 
최적 정책($\pi_*$)을 찾는 것임.

<a id="41-optimal-value"></a>
### 4.1 최적 가치 함수 ($v_*, q_*$)
* $v_*(s) = \max_\pi v_\pi(s)$: 모든 정책 중 상태 $s$에서 얻을 수 있는 최대 가치임.
* $q_*(s, a) = \max_\pi q_\pi(s, a)$: 모든 정책 중 상태 $s$에서 행동 $a$를 취해 얻을 수 있는 최대 가치임.

<a id="42-optimal-policy"></a>
### 4.2 최적 정책 ($\pi_*$)과 벨만 최적 방정식
* **Finding $\pi_*$**: $q_*(s, a)$를 안다면, 최적 행동은 가장 높은 $q_*$ 값을 주는 행동을 100% 확률로 선택하는 것임.
    $$\pi_*(a|s) = 1 \text{ if } a = \text{argmax}_{a \in \mathcal{A}} q_*(s, a) \text{, else } 0$$
* **벨만 최적 방정식 ($v_*$)**: 평균 대신 **최대값(max)**을 취함.
    $$v_*(s) = \max_a q_*(s, a)$$
    $$v_*(s) = \max_a \left( \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_*(s') \right)$$
* **벨만 최적 방정식 ($q_*$ )**:
    $$q_*(s, a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a \max_{a'} q_*(s', a')$$

> **[주의]**
> 벨만 최적 방정식은 **max** 연산자 때문에 비선형(Non-linear)임. 따라서 MRP처럼 역행렬로 한 번에 풀 수 없으며, Value Iteration, Policy Iteration, Q-learning 등의 반복 알고리즘으로 풀어야 함.