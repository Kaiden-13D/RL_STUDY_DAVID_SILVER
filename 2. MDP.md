# 🎓 [Study Note] David Silver 강화학습 제2강: MDP 정리

## 📑 목차
1. [Introduction](#0-introduction)
2. [Step 1. Markov Process (MP)](#1-step-1-markov-process-mp)
3. [Step 2. Markov Reward Process (MRP)](#2-step-2-markov-reward-process-mrp)
    * 2.1 Return ($G_t$)와 할인율 ($\gamma$)
    * 2.2 가치 함수 ($v(s)$)
    * 2.3 벨만 기대 방정식 (Bellman Expectation Equation)
4. [Step 3. Markov Decision Process (MDP)](#3-step-3-markov-decision-process-mdp)
    * 3.1 정책 (Policy, $\pi$)
    * 3.2 두 가지 가치 함수 ($v_{\pi}$, $q_{\pi}$)
    * 3.3 MDP 벨만 기대 방정식
5. [Step 4. Optimal Value & Policy](#4-step-4-optimal-value--policy)
    * 4.1 최적 가치 함수 ($v_{*}$, $q_{*}$)
    * 4.2 최적 정책 ($\pi_{*}$)과 벨만 최적 방정식

---

<a id="0-introduction"></a>
## 0. Introduction
* **MDP 정의**: 강화학습 환경을 수학적으로 공식화하는 도구임.
* **핵심 가정**: 환경이 **Fully Observable**하여 현재 상태($S_t$)가 미래를 결정하는 데 필요한 모든 정보를 담고 있음.
* **범용성**: 거의 모든 RL 문제는 MDP로 공식화 가능함.

---

<a id="1-step-1-markov-process-mp"></a>
## 1. Step 1: Markov Process (MP) - "상태 전이의 기초"
* **정의**: 행동(Action)과 보상(Reward) 없이 상태만 확률적으로 변하는 시스템임.
* **마르코프 성질 (Markov Property)**: "미래는 과거와 독립적이며, 오직 현재에 의해서만 결정됨."
    * $P[S_{t+1} \mid S_{t}] = P[S_{t+1} \mid S_{1}, \dots, S_{t}]$
* **상태 전이 행렬 (State Transition Matrix, $P$)**: 모든 상태에서 다음 상태로 갈 확률을 정리한 $n \times n$ 행렬임.
    * $P_{ss'} = P[S_{t+1} = s' \mid S_{t} = s]$
    * 각 행의 합은 항상 1이며, 대각 성분은 제자리걸음 확률을 의미함.

---

<a id="2-step-2-markov-reward-process-mrp"></a>
## 2. Step 2: Markov Reward Process (MRP) - "가치 평가의 시작"
* **구조**: MP에 보상($R$)과 할인율($\gamma$)이 추가된 형태임.

### 2.1 Return ($G_t$)와 할인율 ($\gamma$)
* **Return ($G_t$)**: 시점 $t$부터 종료까지 받을 누적 보상의 합임.
    * $G_{t} = R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}$
* **할인율 ($\gamma \in [0, 1]$)**: 미래 보상의 현재 가치를 결정함.
    * $\gamma=0$: 근시안적 평가. 당장 받는 보상만 중시함.
    * $\gamma=1$: 원시안적 평가. 미래 보상을 현재와 동일하게 중시함.

### 2.2 가치 함수 (Value Function, $v(s)$)
* **정의**: 상태 $s$에서 시작했을 때 기대할 수 있는 반환값($G_t$)의 평균임.
    * $v(s) = E[G_t | S_t = s]$

### 2.3 벨만 기대 방정식 (Bellman Expectation Equation)
* **재귀적 관계**: 현재 가치 = 즉각 보상 + 할인된 다음 상태 가치의 평균임.
    * $v(s) = E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]$
* **확률 기반 전개**:
    * $v(s) = R_{s} + \gamma \sum_{s' \in S} P_{ss'} v(s')$
* **시스템적 해법 (Matrix Form)**: 
    * $v = R + \gamma P v \implies v = (I - \gamma P)^{-1} R$
    * $O(n^3)$ 복잡도로 인해 소규모 환경에서만 직접 계산 가능함.

---

<a id="3-step-3-markov-decision-process-mdp"></a>
## 3. Step 3: Markov Decision Process (MDP) - "의사결정의 도입"
* 에이전트가 **행동(Action)**을 통해 환경에 개입함.

### 3.1 정책 (Policy, $\pi$)
* **정의**: 상태 $s$에서 행동 $a$를 할 확률 분포임.
    * $\pi(a|s) = P[A_t = a | S_t = s]$
* **특징**: 시간과 무관하게 상태에만 의존하는 **Stationary** 성질을 가짐.
* **MDP to MRP**: 정책 $\pi$가 고정되면 MDP는 다시 MRP로 변환되어 분석 가능함.

### 3.2 두 가지 가치 함수
* **상태 가치 함수 ($v_{\pi}(s)$)**: 정책 $\pi$를 따를 때의 평균 기대 수익임.
    * $v_{\pi}(s) = E_{\pi} [G_t | S_t = s]$
* **행동 가치 함수 ($q_{\pi}(s, a)$)**: 상태 $s$에서 $a$를 하고 나서 정책 $\pi$를 따를 때의 기대 수익임.
    * $q_{\pi}(s, a) = E_{\pi} [G_t | S_t = s, A_t = a]$

### 3.3 MDP 벨만 기대 방정식
* **상호 관계**:
    * $v_{\pi}(s) = \sum_{a \in A} \pi(a|s) q_{\pi}(s, a)$
    * $q_{\pi}(s, a) = R_{s}^{a} + \gamma \sum_{s' \in S} P_{ss'}^{a} v_{\pi}(s')$
* **통합식**:
    * $v_{\pi}(s) = \sum_{a \in A} \pi(a|s) \left(R_{s}^{a} + \gamma \sum_{s' \in S} P_{ss'}^{a} v_{\pi}(s')\right)$

---

<a id="4-step-4-optimal-value--policy"></a>
## 4. Step 4: Optimal Value & Policy - "최적의 길 찾기"

### 4.1 최적 가치 함수 ($v_{*}$, $q_{*}$)
* **$v_{*}(s)$**: 모든 정책 중 상태 $s$에서 얻을 수 있는 최대 평균 수익임.
    * $v_{*}(s) = \max_{\pi} v_{\pi}(s)$
* **$q_{*}(s, a)$**: 모든 정책 중 $(s, a)$에서 얻을 수 있는 최대 평균 수익임.
    * $q_{*}(s, a) = \max_{\pi} q_{\pi}(s, a)$

### 4.2 최적 정책 ($\pi_{*}$)과 벨만 최적 방정식
* **최적 정책 찾기**: 성적표 $q_{*}$에서 가장 높은 값을 주는 행동을 100% 선택함.
    * $\pi_{*}(a|s) = 1$ (if $a = \text{argmax}_{a \in A} q_{*}(s, a)$), else $0$
* **벨만 최적 방정식 ($v_{*}$)**: 평균 대신 **최대값(max)**을 취함.
    * $v_{*}(s) = \max_{a} \left(R_{s}^{a} + \gamma \sum_{s' \in S} P_{ss'}^{a} v_{*}(s')\right)$
* **벨만 최적 방정식 ($q_{*}$)**:
    * $q_{*}(s, a) = R_{s}^{a} + \gamma \sum_{s' \in S} P_{ss'}^{a} \max_{a'} q_{*}(s', a')$

> **⚠️ 주의사항**
> 벨만 최적 방정식은 **max** 연산자 때문에 **비선형(Non-linear)**임. 따라서 역행렬로 풀 수 없으며, Value Iteration 등 반복 알고리즘으로 풀어야 함.