# RL Study - Lecture 1: Introduction to Reinforcement Learning

강화학습 기초 개념과 에이전트 구성 요소 정리함.

---

## 1. 강화학습의 핵심 (The RL Problem)
* **보상 ($R_t$)**: 에이전트가 얼마나 잘하고 있는지 알려주는 **스칼라(Scalar)** 신호임.
* **목표**: 에피소드 끝날 때까지 받는 **누적 보상(Cumulative Reward)을 최대화**하는 정책을 찾는 것임.
* **특징**: 정답(Supervisor)이 없고 오직 보상만 존재하며, 피드백이 지연될 수 있음.

## 2. 에이전트와 환경의 상호작용 (Loop)
1. **에이전트**: 상태 $S_t$를 보고 행동 $A_t$를 취함.
2. **환경**: 행동 $A_t$를 받아 새로운 관찰 $O_{t+1}$과 보상 $R_{t+1}$을 내보냄(Emit).
3. **Observation**: 환경이 제공하는 가시적인 데이터 집합임.

## 3. 상태(State)와 마르코프(Markov)
* **History ($H_t$)**: 지금까지 일어난 모든 $O, R, A$의 시퀀스(로그 데이터)임.
* **State ($S_t$)**: 다음 일을 결정하는 데 쓰는 정보(시스템 컨텍스트)임. $S_t = f(H_t)$.
* **Markov State**: 현재가 주어지면 미래는 과거와 독립적임.
  $$\mathbb{P}[S_{t+1}|S_t] = \mathbb{P}[S_{t+1}|S_1, \dots, S_t]$$
  * **History**와 **Env State**는 정의상 항상 마르코프임. 필요한 정보를 다 갖고 있거나 과거 그 자체이기 때문임.

---

## 4. 에이전트 내부 구성 요소 (Inside Agent)

### 4.1 정책 (Policy, $\pi$)
에이전트의 행동 방식(행동표)임. 상태 $s$를 먹이로 받아 행동 $a$를 뱉음.
* **Deterministic**: $a = \pi(s)$ (딱 정해진 행동).
* **Stochastic**: $\pi(a|s) = \mathbb{P}[A_t = a | S_t = s]$ (각 행동을 취할 확률 배열 리턴).

### 4.2 가치 함수 (Value Function, $v_{\pi}$)
현재 상태가 나중에 얼마나 좋을지 예측하는 성적표임.
$$
v_{\pi}(s) = \mathbb{E}_{\pi} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \mid S_t = s]
$$

* **감마**($\gamma$): 할인율($0 \leq \gamma \leq 1$). 무한 루프 방지(수렴), 미래 불확실성 반영, 즉각적 보상 선호 등을 위해 사용함.


### 4.3 모델 (Model)
에이전트 머릿속의 가상 시뮬레이터임.
* **Transition ($\mathcal{P}$)**: 행동 시 다음 상태 예측.
* **Reward ($\mathcal{R}$)**: 행동 시 당장 받을 즉각적 보상 예측.

---

## 5. 에이전트 분류 및 주요 개념

### 5.1 분류 (Taxonomy)
* **Value-Based**: 가치 함수(성적표)만 가짐. 정책은 따로 없고 점수 높은 데로 이동함.
* **Policy-Based**: 정책(행동표)만 가짐.
* **Actor-Critic**: 둘 다 가짐. 행동표대로 움직이고 성적표로 평가함.
* **Model-Free vs Model-Based**: 세상 지도가 없어서 경험으로 조지는지($O$), 머릿속 시뮬레이터로 계획(Planning)을 세우는지 차이임.

### 5.2 Exploration vs Exploitation
* **Exploration (탐험)**: 정보 모으기(투자). 새로운 시도로 데이터 쌓기.
* **Exploitation (이용)**: 아는 정보로 보상 최대화(수익). 가치 함수가 알려주는 최고점 따먹기.
* **둘 다 실제로 Action을 취하는 것임.** 차이는 "성적표(Value)를 믿었냐" vs "모험을 했냐"의 로직 차이임.

### 5.3 Prediction vs Control
* **Prediction**: 주어진 정책으로 미래 가치(Value)를 평가함 (그리드월드의 숫자들이 이 결과임).
* **Control**: 미래를 최적화해서 가장 좋은 최적 정책($\pi^*$)을 찾음.